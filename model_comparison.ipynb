{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c69ea50",
   "metadata": {},
   "source": [
    "# Model Comparison Analysis\n",
    "\n",
    "This notebook aggregates results from `./runs/<variant>/` and produces comprehensive visualizations:\n",
    "- Learning curves for Top-1 accuracy and validation loss\n",
    "- Best Top-1 and Top-5 accuracy bar charts\n",
    "- Accuracy vs training time scatter plot\n",
    "- Family averages bar chart\n",
    "- Per-class comparison bars for top-performing models\n",
    "\n",
    "All outputs are saved to `./comparison_outputs/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999bc7df",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54aac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable inline plotting for Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default figure size and style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8207b0",
   "metadata": {},
   "source": [
    "## 2. Define Configuration and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd867d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANTS = [\n",
    "    \"r18_base\",\"r18_plus\",\"r34_base\",\"r34_plus\",\n",
    "    \"efficientnet_b0\",\"efficientnet_b1\",\"efficientnet_b2\",\"densenet121\"\n",
    "]\n",
    "\n",
    "RUNS = Path(\"./runs\")\n",
    "OUT = Path(\"./comparison_outputs\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_JSON = Path(\"experiment_results.json\")\n",
    "\n",
    "def find_metrics_path(variant):\n",
    "    \"\"\"Find the metrics log file for a given variant\"\"\"\n",
    "    for p in [RUNS/variant/\"metrics_log.tsv\", RUNS/variant/variant/\"metrics_log.tsv\"]:\n",
    "        if p.exists(): return p\n",
    "    return None\n",
    "\n",
    "def find_summary_path(variant):\n",
    "    \"\"\"Find the model summary file for a given variant\"\"\"\n",
    "    for p in [RUNS/variant/\"model_summary.json\", RUNS/variant/variant/\"model_summary.json\"]:\n",
    "        if p.exists(): return p\n",
    "    return None\n",
    "\n",
    "def find_per_class_path(variant):\n",
    "    \"\"\"Find the per-class metrics file for a given variant\"\"\"\n",
    "    for p in [RUNS/variant/\"per_class_metrics.csv\", RUNS/variant/variant/\"per_class_metrics.csv\"]:\n",
    "        if p.exists(): return p\n",
    "    return None\n",
    "\n",
    "def bucket(v):\n",
    "    \"\"\"Categorize variants into model families\"\"\"\n",
    "    if v in [\"r18_base\",\"r34_base\"]: return \"ResNet (base)\"\n",
    "    if v in [\"r18_plus\",\"r34_plus\"]: return \"ResNet (plus)\"\n",
    "    if v.startswith(\"efficientnet\"): return \"EfficientNet\"\n",
    "    if v.startswith(\"densenet\"): return \"DenseNet\"\n",
    "    return \"Other\"\n",
    "\n",
    "print(f\"âœ“ Configuration loaded - tracking {len(VARIANTS)} variants\")\n",
    "print(f\"âœ“ Output directory: {OUT.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544f82cf",
   "metadata": {},
   "source": [
    "## 3. Load Training Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load times if present\n",
    "times_by_variant = {}\n",
    "if RESULTS_JSON.exists():\n",
    "    with open(RESULTS_JSON) as f:\n",
    "        data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            for r in data:\n",
    "                if isinstance(r, dict) and r.get(\"variant\"):\n",
    "                    times_by_variant[r[\"variant\"]] = r.get(\"training_time\", None)\n",
    "\n",
    "print(f\"âœ“ Training times loaded for {len(times_by_variant)} variants\")\n",
    "if times_by_variant:\n",
    "    print(\"Available timing data:\", list(times_by_variant.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4221d5",
   "metadata": {},
   "source": [
    "## 4. Collect Metrics from All Model Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeefb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics\n",
    "per_epoch = {}\n",
    "for v in VARIANTS:\n",
    "    mp = find_metrics_path(v)\n",
    "    if mp is not None:\n",
    "        try:\n",
    "            df = pd.read_csv(mp, sep=\"\\t\")\n",
    "            per_epoch[v] = df\n",
    "            print(f\"âœ“ Loaded metrics for {v}: {len(df)} epochs\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not read metrics for {v}: {e}\")\n",
    "    else:\n",
    "        print(f\"[INFO] No metrics file found for {v}\")\n",
    "\n",
    "print(f\"\\nâœ“ Successfully loaded metrics for {len(per_epoch)} variants\")\n",
    "print(\"Available variants:\", list(per_epoch.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb62ac0",
   "metadata": {},
   "source": [
    "## 5. Generate Top-1 Accuracy Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "any_plot = False\n",
    "colors = plt.cm.tab10(range(len(per_epoch)))\n",
    "\n",
    "for i, (v, df) in enumerate(per_epoch.items()):\n",
    "    if \"epoch\" in df.columns and \"top1\" in df.columns:\n",
    "        plt.plot(df[\"epoch\"], df[\"top1\"], label=v, linewidth=2, \n",
    "                marker='o', markersize=4, color=colors[i])\n",
    "        any_plot = True\n",
    "\n",
    "plt.title(\"Top-1 Accuracy vs Epoch\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Top-1 Accuracy (%)\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "if any_plot:\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT/\"learning_curves_top1.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Top-1 accuracy data available for plotting\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed96de",
   "metadata": {},
   "source": [
    "## 6. Generate Validation Loss Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7270bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "any_plot = False\n",
    "\n",
    "for i, (v, df) in enumerate(per_epoch.items()):\n",
    "    if \"epoch\" in df.columns and \"val_loss\" in df.columns:\n",
    "        plt.plot(df[\"epoch\"], df[\"val_loss\"], label=v, linewidth=2, \n",
    "                marker='o', markersize=4, color=colors[i])\n",
    "        any_plot = True\n",
    "\n",
    "plt.title(\"Validation Loss vs Epoch\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Validation Loss\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "if any_plot:\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT/\"learning_curves_val_loss.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No validation loss data available for plotting\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93124ba0",
   "metadata": {},
   "source": [
    "## 7. Create Performance Summary and Best Accuracy Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary\n",
    "rows = []\n",
    "for v, df in per_epoch.items():\n",
    "    row = {\"variant\": v}\n",
    "    row[\"best_top1\"] = float(df[\"top1\"].max()) if \"top1\" in df.columns else float(\"nan\")\n",
    "    row[\"best_top5\"] = float(df[\"top5\"].max()) if \"top5\" in df.columns else float(\"nan\")\n",
    "    rows.append(row)\n",
    "\n",
    "perf = pd.DataFrame(rows).sort_values(\"best_top1\", ascending=False)\n",
    "perf.to_csv(OUT/\"summary_performance.csv\", index=False)\n",
    "\n",
    "print(\"Performance Summary:\")\n",
    "display(perf.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Top-1 Accuracy Bar Chart\n",
    "if not perf.empty and perf[\"best_top1\"].notna().any():\n",
    "    plt.figure(figsize=(12,8))\n",
    "    bars = plt.bar(perf[\"variant\"], perf[\"best_top1\"], \n",
    "                   color=plt.cm.viridis(range(len(perf))), alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, perf[\"best_top1\"]):\n",
    "        if not pd.isna(value):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.title(\"Best Top-1 Accuracy by Model\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Model Variant\", fontsize=12)\n",
    "    plt.ylabel(\"Best Top-1 Accuracy (%)\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT/\"best_top1_bar.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Top-1 accuracy data available for bar chart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8769211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Top-5 Accuracy Bar Chart  \n",
    "if not perf.empty and perf[\"best_top5\"].notna().any():\n",
    "    plt.figure(figsize=(12,8))\n",
    "    valid_top5 = perf.dropna(subset=['best_top5'])\n",
    "    bars = plt.bar(valid_top5[\"variant\"], valid_top5[\"best_top5\"], \n",
    "                   color=plt.cm.plasma(range(len(valid_top5))), alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, valid_top5[\"best_top5\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.title(\"Best Top-5 Accuracy by Model\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Model Variant\", fontsize=12)\n",
    "    plt.ylabel(\"Best Top-5 Accuracy (%)\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT/\"best_top5_bar.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Top-5 accuracy data available for bar chart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35d911",
   "metadata": {},
   "source": [
    "## 8. Generate Accuracy vs Training Time Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accuracy vs time data\n",
    "rows = []\n",
    "for v, df in per_epoch.items():\n",
    "    if \"top1\" in df.columns:\n",
    "        best_top1 = float(df[\"top1\"].max())\n",
    "        t = times_by_variant.get(v, None)\n",
    "        if t is not None:\n",
    "            rows.append({\"variant\": v, \"best_top1\": best_top1, \"minutes\": t/60.0})\n",
    "\n",
    "trade = pd.DataFrame(rows).sort_values(\"best_top1\", ascending=False)\n",
    "trade.to_csv(OUT/\"accuracy_vs_time.csv\", index=False)\n",
    "\n",
    "if not trade.empty:\n",
    "    plt.figure(figsize=(10,8))\n",
    "    scatter = plt.scatter(trade[\"minutes\"], trade[\"best_top1\"], \n",
    "                         s=120, c=range(len(trade)), cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    # Add annotations\n",
    "    for _, r in trade.iterrows():\n",
    "        plt.annotate(r[\"variant\"], (r[\"minutes\"], r[\"best_top1\"]), \n",
    "                    xytext=(5,5), textcoords=\"offset points\", \n",
    "                    fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.title(\"Best Top-1 vs Training Time\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Training Time (minutes)\", fontsize=12)\n",
    "    plt.ylabel(\"Best Top-1 Accuracy (%)\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, label='Performance Rank')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT/\"accuracy_vs_time_scatter.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Accuracy vs Time Summary:\")\n",
    "    display(trade.round(2))\n",
    "else:\n",
    "    print(\"No timing data available for scatter plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afdeccf",
   "metadata": {},
   "source": [
    "## 9. Create Family Average Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd486227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create family analysis\n",
    "fam_rows = []\n",
    "for v, df in per_epoch.items():\n",
    "    if \"top1\" in df.columns:\n",
    "        fam_rows.append({\"family\": bucket(v), \"variant\": v, \"best_top1\": float(df[\"top1\"].max())})\n",
    "\n",
    "fam = pd.DataFrame(fam_rows)\n",
    "fam.to_csv(OUT/\"family_raw.csv\", index=False)\n",
    "\n",
    "if not fam.empty:\n",
    "    avg = fam.groupby(\"family\", as_index=False)[\"best_top1\"].mean().sort_values(\"best_top1\", ascending=False)\n",
    "    avg.to_csv(OUT/\"family_avg.csv\", index=False)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    bars = plt.bar(avg[\"family\"], avg[\"best_top1\"], \n",
    "                   color=plt.cm.Set3(range(len(avg))), alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, avg[\"best_top1\"]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.title(\"Average Best Top-1 by Model Family\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Family\", fontsize=12)\n",
    "    plt.ylabel(\"Average Best Top-1 (%)\", fontsize=12)\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT/\"family_averages_bar.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Family Performance Summary:\")\n",
    "    display(avg.round(2))\n",
    "else:\n",
    "    print(\"No family data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc9bc9",
   "metadata": {},
   "source": [
    "## 10. Generate Per-Class Accuracy Comparison for Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top-2 variants by best_top1\n",
    "top2 = list(perf[\"variant\"].head(2).values) if not perf.empty else []\n",
    "pcs_loaded = 0\n",
    "\n",
    "print(f\"Analyzing per-class metrics for top 2 models: {top2}\")\n",
    "\n",
    "for v in top2:\n",
    "    p = find_per_class_path(v)\n",
    "    if p is None: \n",
    "        print(f\"No per-class metrics file found for {v}\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        # Use recall as a proxy if accuracy not present\n",
    "        if \"accuracy\" in df.columns:\n",
    "            series = df.set_index(df.columns[0])[\"accuracy\"]\n",
    "            metric_name = \"Accuracy\"\n",
    "        elif \"recall\" in df.columns:\n",
    "            series = df.set_index(df.columns[0])[\"recall\"]\n",
    "            metric_name = \"Recall\"\n",
    "        else:\n",
    "            series = None\n",
    "            \n",
    "        if series is None or series.empty:\n",
    "            print(f\"No usable metrics found in per-class file for {v}\")\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(12,6))\n",
    "        bars = plt.bar(range(len(series.values)), series.values, \n",
    "                      color=plt.cm.viridis(range(len(series.values))), alpha=0.7)\n",
    "        \n",
    "        plt.title(f\"Per-Class {metric_name} â€” {v}\", fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Class (index order)\", fontsize=12)\n",
    "        plt.ylabel(f\"{metric_name} Score\", fontsize=12)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add some statistics as text\n",
    "        mean_score = series.mean()\n",
    "        std_score = series.std()\n",
    "        plt.text(0.02, 0.98, f'Mean: {mean_score:.3f}\\\\nStd: {std_score:.3f}', \n",
    "                transform=plt.gca().transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT/f\"per_class_{v}.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        pcs_loaded += 1\n",
    "        \n",
    "        print(f\"âœ“ Loaded per-class metrics for {v}: {len(series)} classes, mean {metric_name.lower()}: {mean_score:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Per-class metrics for {v}: {e}\")\n",
    "\n",
    "print(f\"Successfully loaded per-class data for {pcs_loaded} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84748e54",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8145625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ“Š Analyzed {len(per_epoch)} model variants\")\n",
    "print(f\"ðŸ“ Outputs saved to: {OUT.resolve()}\")\n",
    "print(\"\\\\nGenerated visualizations:\")\n",
    "print(\"  âœ“ Learning curves (Top-1 accuracy)\")\n",
    "print(\"  âœ“ Learning curves (Validation loss)\")\n",
    "print(\"  âœ“ Best Top-1 accuracy comparison\")\n",
    "if not perf.empty and perf[\"best_top5\"].notna().any():\n",
    "    print(\"  âœ“ Best Top-5 accuracy comparison\")\n",
    "if not trade.empty:\n",
    "    print(\"  âœ“ Accuracy vs training time scatter\")\n",
    "print(\"  âœ“ Model family averages\")\n",
    "if pcs_loaded > 0:\n",
    "    print(f\"  âœ“ Per-class analysis for top {pcs_loaded} models\")\n",
    "print(\"\\\\nData files saved:\")\n",
    "print(\"  â€¢ summary_performance.csv\")\n",
    "print(\"  â€¢ accuracy_vs_time.csv\")  \n",
    "print(\"  â€¢ family_raw.csv\")\n",
    "print(\"  â€¢ family_avg.csv\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
